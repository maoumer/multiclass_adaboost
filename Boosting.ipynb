{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting - Mohammed Adib Oumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restructure images and prepare training and test(or validation) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when running the below cell again with diff dimension and saving to a folder, run this before.\n",
    "# !rmdir /s /q xray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images(dimension):\n",
    "    imgs = np.zeros((1,np.prod(dimension)+1))\n",
    "    train, test = imgs,imgs\n",
    "\n",
    "    in_directory = [\"tumor_mri/test/glioma\",\"tumor_mri/test/meningioma\",\\\n",
    "                \"tumor_mri/test/notumor\",\"tumor_mri/test/pituitary\",\\\n",
    "                \"tumor_mri/train/glioma\",\"tumor_mri/train/meningioma\",\\\n",
    "                \"tumor_mri/train/notumor\",\"tumor_mri/train/pituitary\"]\n",
    "\n",
    "    out_directory = [\"mri/test/glioma\",\"mri/test/meningioma\",\\\n",
    "                \"mri/test/notumor\",\"mri/test/pituitary\",\\\n",
    "                \"mri/train/glioma\",\"mri/train/meningioma\",\\\n",
    "                \"mri/train/notumor\",\"mri/train/pituitary\"]\n",
    "\n",
    "    valid_images = [\".jpeg\",\".jpg\",\".png\"]\n",
    "\n",
    "    for i in range(len(in_directory)):\n",
    "        if not os.path.exists(out_directory[i]):\n",
    "            # os.makedirs(out_directory[i])\n",
    "            imgs = np.zeros((1,np.prod(dimension)+1))\n",
    "            group = in_directory[i].split(\"/\")[1:]\n",
    "            if (group[-1] == \"glioma\"):\n",
    "                label = 0.\n",
    "            elif (group[-1] == \"meningioma\"):\n",
    "                label = 1.\n",
    "            elif (group[-1] == \"notumor\"):\n",
    "                label = 2. \n",
    "            else:\n",
    "                label = 3.\n",
    "            label = np.array(label).reshape((1,1))\n",
    "\n",
    "            for f in os.listdir(in_directory[i]):\n",
    "                ext = os.path.splitext(f)[1]\n",
    "                if ext.lower() not in valid_images:\n",
    "                    continue\n",
    "                img = Image.open(os.path.join(in_directory[i],f)).convert(\"L\")\n",
    "                img = img.resize(size=(dimension[0],dimension[1])) # resize\n",
    "\n",
    "                # img.save(f\"{out_directory[i]}/{f}\") # optional - save restructed image to folder\n",
    "                img = np.array(img).reshape((1,np.prod(dimension)))\n",
    "                imgs = np.vstack((imgs,np.hstack((img,label))))\n",
    "\n",
    "            imgs = imgs[1:,:]\n",
    "            if (group[0] == \"train\"):\n",
    "                train = np.vstack((train,imgs))\n",
    "            else:\n",
    "                test = np.vstack((test,imgs))\n",
    "\n",
    "    train, test = train[1:,:], test[1:,:]\n",
    "    train[:,:-1] /= 255\n",
    "    test[:,:-1] /= 255\n",
    "    # remove the below if you shuffle elsewhere\n",
    "    train=np.random.permutation(train)\n",
    "    test = np.random.permutation(test)\n",
    "    return train, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5712, 4097), (1311, 4097))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# by inspection of files, min Width = 384, min Height = 127\n",
    "dimension = np.array([64,64]) # will increase this later \n",
    "train, test = process_images(dimension)\n",
    "train.shape, test.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Naive Bayes weak classifier with Adaptive Boositng (AdaBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayespost(data,px,py):\n",
    "    # we need to incorporate the prior probability p(y) since p(y|x) is\n",
    "    # proportional to p(x|y) p(y)\n",
    "    data = data.reshape((len(data),1))\n",
    "    # work in log scale\n",
    "    logpx = np.log(px)\n",
    "    logpxneg = np.log(1-px)\n",
    "    logpy = np.log(py)\n",
    "    logpost = logpy\n",
    "    logpost += (logpx * data + logpxneg * (1-data)).sum(0)\n",
    "    # normalize to prevent overflow or underflow by subtracting the largest value\n",
    "    logpost -= np.max(logpost)\n",
    "    # get the estimate back\n",
    "    post = np.exp(logpost)\n",
    "    post /= np.sum(post)\n",
    "    return post\n",
    "\n",
    "def gaussNB(data,means,vars,n_classes,py):\n",
    "    x = np.ones(len(data))\n",
    "    for i in range(n_classes):\n",
    "        x0 = (np.log(1/np.sqrt(2*3.1415*vars[:,i]))\\\n",
    "                -0.5*((data[:,:-1] - means[:,i])**2)/vars[:,i]).sum(1)\n",
    "        x = np.vstack((x,x0))\n",
    "    x = (x[1:,:]).T\n",
    "    logpy = np.log(py)\n",
    "    pred_label = (x+logpy).argmax(1)\n",
    "    return pred_label\n",
    "\n",
    "def Loss(y,yhat):\n",
    "    return (y!=yhat).sum()/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# yyy = OneHotEncoder(sparse=False).fit_transform(test[:,-1].reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class data_iterable(object):\n",
    "    def __init__(self, features, labels, batch_size):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.num_examples = len(features)\n",
    "        self.indices = list(range(self.num_examples))\n",
    "        # The examples are read at random, in no particular order\n",
    "        random.shuffle(self.indices)\n",
    "    \n",
    "    def data_iterator(self):\n",
    "        for i in range(0, self.num_examples, self.batch_size):\n",
    "            batch_indices = torch.tensor(self.indices[i: min(i + self.batch_size, self.num_examples)])\n",
    "            yield self.features[batch_indices], self.labels[batch_indices]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.data_iterator() \n",
    "\n",
    "class LogRegression():   \n",
    "    def onehotencode(self,y):\n",
    "        classes = np.sort(np.unique(y))\n",
    "        return (y.reshape((len(y),1)) == classes).astype(float)\n",
    "    \n",
    "    def softmax(self,Z,axis=1):\n",
    "        # Z_max = np.max(Z, axis = 1, keepdims = True)[0]\n",
    "        # Z_exp = np.exp(Z - Z_max)\n",
    "        # partition = Z_exp.sum(1, keepdims=True)\n",
    "        # return Z_exp / partition  # The broadcasting mechanism is applied here\n",
    "        Z_sum = np.sum(np.exp(Z),axis=axis,keepdims=True)\n",
    "        return np.exp(Z)/Z_sum\n",
    "    \n",
    "    def loss(self,X, y, W): # y one hot encoded\n",
    "        N = X.shape[0]\n",
    "        loss = 1/N * (np.trace(X @ W @ y.T) + np.sum(np.log(np.sum(np.exp(X @ W), axis=1))))\n",
    "        return loss\n",
    "    \n",
    "    def gradient(self,X, y, W, mu): # y one hot encoded\n",
    "        Z = -X @ W\n",
    "        P = self.softmax(Z)\n",
    "        N = X.shape[0]\n",
    "        gd = 1/N * (X.T @ (y - P)) + 2 * mu * W\n",
    "        return gd\n",
    "\n",
    "    def gradient_descent(self,X, y, max_iter, lr, mu):\n",
    "        y_onehot = self.onehotencode(y)\n",
    "        W = np.zeros((X.shape[1], y_onehot.shape[1]))\n",
    "        step = 0\n",
    "        self.step_lst =list()\n",
    "        self.loss_lst = list()\n",
    "        self.W_lst = list()\n",
    "    \n",
    "        while step < max_iter:\n",
    "            step += 1\n",
    "            W -= lr * self.gradient(X, y_onehot, W, mu)\n",
    "            self.step_lst.append(step)\n",
    "            self.W_lst.append(W)\n",
    "            self.loss_lst.append(self.loss(X, y_onehot, W))\n",
    "            # if step%1000 == 0:\n",
    "            #     print(\"step\",step)\n",
    "        return W\n",
    "\n",
    "    def fit(self,X,y,max_iter=1000,lr=0.1,mu=0):\n",
    "        self.max_iter = max_iter\n",
    "        self.lr = lr\n",
    "        self.mu = mu\n",
    "        self.X = np.hstack((np.ones((len(X),1)),X))\n",
    "        self.y = y\n",
    "        self.W = self.gradient_descent(self.X,self.y,self.max_iter,self.lr,self.mu)\n",
    "    \n",
    "    def predict(self,X):\n",
    "        X = np.hstack((np.ones((len(X),1)),X))\n",
    "        Z = -X @ self.W\n",
    "        P = self.softmax(Z)\n",
    "        return P.argmax(1)\n",
    " \n",
    "\n",
    "# batch_size = 256\n",
    "# train_iter = data_iterable(train_images, train_labels, batch_size)\n",
    "# test_iter = data_iterable(test_images, test_labels, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "Training Accuracy: 65.53%\n",
      "Training Accuracy: 61.1%\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.datasets import load_iris\n",
    "# X = load_iris().data\n",
    "# Y = load_iris().target\n",
    "# model = LogRegression()\n",
    "# model.fit(X, Y,max_iter=100,mu=0.01)\n",
    "# Loss(model.predict(X),Y)\n",
    "\n",
    "LR = LogRegression()\n",
    "LR.fit(train[:,:-1],train[:,-1],500,mu=0.01)\n",
    "pred = LR.predict(train[:,:-1])\n",
    "err_rate = Loss(train[:,-1],pred)\n",
    "print(f\"Training Accuracy: {100*(1-err_rate):.{4}}%\")\n",
    "pred = LR.predict(test[:,:-1])  \n",
    "err_rate = Loss(test[:,-1],pred)\n",
    "print(f\"Training Accuracy: {100*(1-err_rate):.{4}}%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 86.03%\n",
      "Training Accuracy: 78.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moham\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# model = LogisticRegression(C=0.01,multi_class='multinomial')\n",
    "# model.fit(X, Y)\n",
    "# Loss(model.predict(X),Y)\n",
    "\n",
    "LR = LogisticRegression(C=0.01,multi_class='multinomial')\n",
    "LR.fit(train[:,:-1],train[:,-1])\n",
    "pred = LR.predict(train[:,:-1])\n",
    "err_rate = Loss(train[:,-1],pred)\n",
    "print(f\"Training Accuracy: {100*(1-err_rate):.{4}}%\")\n",
    "pred = LR.predict(test[:,:-1])  \n",
    "err_rate = Loss(test[:,-1],pred)\n",
    "print(f\"Training Accuracy: {100*(1-err_rate):.{4}}%\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pure naive classifier (for comparison) - single learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 61.85%\n",
      "Testing Accuracy: 57.51%\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = train[:,:-1], train[:,-1]\n",
    "idx0 = (y_train==0.)\n",
    "idx1 = (y_train==1.)\n",
    "idx2 = (y_train==2.)\n",
    "idx3 = (y_train==3.)\n",
    "\n",
    "ycount = np.ones((4))\n",
    "ycount[0],ycount[1],ycount[2],ycount[3] = idx0.sum(),idx1.sum(),idx2.sum(),idx3.sum()\n",
    "py = ycount / ycount.sum()\n",
    "\n",
    "means = np.ones((np.prod(dimension),4))\n",
    "vars = np.ones((np.prod(dimension),4))\n",
    "means[:,0] = X_train[idx0,:].mean(axis=0)\n",
    "means[:,1] = X_train[idx1,:].mean(axis=0)\n",
    "means[:,2] = X_train[idx2,:].mean(axis=0)\n",
    "means[:,3] = X_train[idx3,:].mean(axis=0)\n",
    "vars[:,0] = X_train[idx0,:].var(axis=0)\n",
    "vars[:,1] = X_train[idx1,:].var(axis=0)\n",
    "vars[:,2] = X_train[idx2,:].var(axis=0)\n",
    "vars[:,3] = X_train[idx3,:].var(axis=0)\n",
    "vars += 1e-9 * vars.max()\n",
    "\n",
    "#train\n",
    "pred = gaussNB(train,means,vars,4,py)\n",
    "err_rate = Loss(train[:,-1],pred)\n",
    "print(f\"Training Accuracy: {100*(1-err_rate):.{4}}%\")\n",
    "\n",
    "#test\n",
    "pred = gaussNB(test,means,vars,4,py)\n",
    "err_rate = Loss(test[:,-1],pred)\n",
    "print(f\"Testing Accuracy: {100*(1-err_rate):.{4}}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Unused code\n",
    "##train\n",
    "# x0 = (np.log(1/np.sqrt(2*3.1415*vars[:,0]))\\\n",
    "#         -0.5*((train[:,:-1] - means[:,0])**2)/vars[:,0]).sum(1)\n",
    "# x1 = (np.log(1/np.sqrt(2*3.1415*vars[:,1]))\\\n",
    "#         -0.5*((train[:,:-1] - means[:,1])**2)/vars[:,1]).sum(1)\n",
    "# x2 = (np.log(1/np.sqrt(2*3.1415*vars[:,2]))\\\n",
    "#         -0.5*((train[:,:-1] - means[:,2])**2)/vars[:,2]).sum(1)\n",
    "# x3 = (np.log(1/np.sqrt(2*3.1415*vars[:,3]))\\\n",
    "#         -0.5*((train[:,:-1] - means[:,3])**2)/vars[:,3]).sum(1)\n",
    "# logpy = np.log(py)\n",
    "# x = np.vstack((x0,np.vstack((x1,np.vstack((x2,x3)))))).T\n",
    "# pred = (x+logpy).argmax(1)\n",
    "\n",
    "\n",
    "\n",
    "## in fit function\n",
    "# self.nb = GaussianNB()\n",
    "# self.nb.fit(X_train, y_train)\n",
    "# self.pred_label2 = self.nb.predict(X_train)\n",
    "# print((self.pred_label==self.pred_label2).all())\n",
    "\n",
    "# err2 = np.dot(err_idx, self.probs[:,-1])/self.probs[:,-1].sum() + 1e-12\n",
    "\n",
    "# if ctr != 0:\n",
    "    # self.pred_label = (sum((pred == np.array([np.arange(self.num_classes)]).T) * al \\\n",
    "    #         for pred, al in zip(self.pred_labels.T,self.alphas)) \\\n",
    "    #         + alpha*(self.pred_label == np.array([np.arange(self.num_classes)]).T)).argmax(0)\n",
    "\n",
    "    # err_idx = (y_train != self.pred_label)\n",
    "    # err = np.dot(err_idx, self.probs[:,-1])/self.probs[:,-1].sum() + 1e-10\n",
    "    # assert err < (1 - 1 / self.num_classes)\n",
    "    # alpha = np.log((1-err)/err) + np.log(self.num_classes-1)\n",
    "\n",
    "## in predict function\n",
    "# pred_label = self.nb.predict(data[:,:-1])\n",
    "\n",
    "## final prediction method\n",
    "# predictions = np.ones((len(data),len(self.alphas),self.num_classes))\n",
    "# for k in range(self.num_classes):\n",
    "#         predictions[:,:,k] = (pred_labels == k)\n",
    "# final_pred2 = ((self.alphas.reshape((1,len(self.alphas),1))*predictions).sum(1)).argmax(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Naive Bayes accuracy: 61.85%\n",
      "Test Naive Bayes accuracy: 57.51%\n"
     ]
    }
   ],
   "source": [
    "# Compare to scikit\n",
    "nb = GaussianNB()\n",
    "nb.fit(train[:,:-1], train[:,-1])\n",
    "print(\"Train Naive Bayes accuracy: %.2f%%\" %(100*nb.score(train[:,:-1], train[:,-1])))\n",
    "print(\"Test Naive Bayes accuracy: %.2f%%\" %(100*nb.score(test[:,:-1], test[:,-1])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaboost based on Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost:\n",
    "\n",
    "    # initialization\n",
    "    def __init__(self):\n",
    "        self.num_classes = 4\n",
    "\n",
    "\n",
    "    def fit(self,data,dimension,estimator,rand,max_iter):\n",
    "\n",
    "        self.errs = np.array(0.) \n",
    "        self.alphas = np.array(0.) \n",
    "        self.pred_labels = np.ones((len(data),1))\n",
    "\n",
    "        self.prob = 1/len(data)*np.ones((len(data),1)) #initialize weights to 1/N uniformly\n",
    "        self.probs = np.ones((len(data),1))\n",
    "        self.probs = np.hstack((self.probs,self.prob))\n",
    "        self.max_iter = max_iter\n",
    "        self.estimator = estimator\n",
    "        self.gnbs,self.mnbs,self.dcts,self.lrs = list(),list(),list(),list()\n",
    "        self.dimension = np.prod(dimension)\n",
    "        self.py = np.ones((self.num_classes,self.max_iter))\n",
    "        self.px = np.ones((self.dimension,self.num_classes,self.max_iter))\n",
    "        self.means = np.ones((np.prod(dimension),self.num_classes,self.max_iter))\n",
    "        self.vars = np.ones((np.prod(dimension),self.num_classes,self.max_iter))\n",
    "\n",
    "        # several learners\n",
    "        for ctr in range(self.max_iter):\n",
    "            ycount = np.zeros(self.num_classes)\n",
    "            # get samples of training data with replacement and find px, py \n",
    "            # random.seed(0)    \n",
    "            # idx = random.choices(np.arange(len(self.probs[:,-1])), k =len(data), weights = list(self.probs[:,-1]/self.prob[:,-1].min()))\n",
    "            np.random.seed(rand)    \n",
    "            idx = np.random.choice(len(self.probs[:,-1]), size =len(data), replace = True, p = self.probs[:,-1])\n",
    "            X_train, y_train = data[idx,:-1], data[idx,-1]\n",
    "\n",
    "            # fit (get mean, var, py)\n",
    "            idx0 = (y_train==0.) #glioma\n",
    "            idx1 = (y_train==1.) #meningioma\n",
    "            idx2 = (y_train==2.) #notumor\n",
    "            idx3 = (y_train==3.) #pituitary\n",
    "            \n",
    "            ycount[0],ycount[1],ycount[2],ycount[3] = idx0.sum(),idx1.sum(),idx2.sum(),idx3.sum()\n",
    "            ycount += 1.\n",
    "            self.py[:,ctr] = ycount / ycount.sum()\n",
    "            if self.estimator == \"Gaussian\":\n",
    "                var_smoothing = 5e-11\n",
    "                self.means[:,0,ctr] = X_train[idx0,:].mean(axis=0)\n",
    "                self.means[:,1,ctr] = X_train[idx1,:].mean(axis=0)\n",
    "                self.means[:,2,ctr] = X_train[idx2,:].mean(axis=0)\n",
    "                self.means[:,3,ctr] = X_train[idx3,:].mean(axis=0)\n",
    "                self.vars[:,0,ctr] = X_train[idx0,:].var(axis=0)+var_smoothing\n",
    "                self.vars[:,1,ctr] = X_train[idx1,:].var(axis=0)+var_smoothing\n",
    "                self.vars[:,2,ctr] = X_train[idx2,:].var(axis=0)+var_smoothing\n",
    "                self.vars[:,3,ctr] = X_train[idx3,:].var(axis=0)+var_smoothing\n",
    "                # self.vars[:,:,ctr] += var_smoothing * self.vars[:,:,ctr].max()\n",
    "\n",
    "                self.pred_label = gaussNB(data,self.means[:,:,ctr],self.vars[:,:,ctr],self.num_classes,self.py[:,ctr])\n",
    "                err_idx = (data[:,-1] != self.pred_label)\n",
    "\n",
    "                ### scikit\n",
    "                # estim = GaussianNB()\n",
    "                # estim.fit(X_train,y_train)\n",
    "                # self.pred_label = estim.predict(data[:,:-1])\n",
    "                # err_idx = (data[:,-1] != self.pred_label)\n",
    "\n",
    "                self.gnbs.append(estim)\n",
    "\n",
    "            elif self.estimator == \"Multinomial\":\n",
    "                ## For 0 to 1 pixel images (treating it as )\n",
    "                # xcount = np.ones((self.dimension,self.num_classes)) # Laplace smoothing\n",
    "                # xcount[:,0] += X_train[idx0,:].sum(axis=0)\n",
    "                # xcount[:,1] += X_train[idx1,:].sum(axis=0)\n",
    "                # xcount[:,2] += X_train[idx2,:].sum(axis=0)\n",
    "                # xcount[:,3] += X_train[idx3,:].sum(axis=0)\n",
    "                # self.px[:,:,ctr] = (xcount / ycount.reshape(1,self.num_classes))#broadcasting\n",
    "                \n",
    "                # pred = np.ones((len(data),self.num_classes))\n",
    "                # for i in range(len(data)):\n",
    "                #     pred[i,:] = bayespost(data[i,:-1],self.px[:,:,ctr],self.py[:,ctr])\n",
    "                # self.pred_label = pred.argmax(1)\n",
    "                # err_idx = (y_train != self.pred_label)\n",
    "                \n",
    "                ###\n",
    "                estim = MultinomialNB()\n",
    "                estim.fit(X_train,y_train)\n",
    "                self.pred_label = estim.predict(data[:,:-1])\n",
    "                err_idx = (data[:,-1] != self.pred_label)\n",
    "                self.mnbs.append(estim)\n",
    "            \n",
    "            elif self.estimator == \"Logistic\":\n",
    "                estim = LogRegression()\n",
    "                # estim.fit(data[:,:-1], data[:,-1],sample_weight=self.probs[:,-1])\n",
    "                estim.fit(X_train,y_train,500,0.1,0.01)\n",
    "                self.pred_label = estim.predict(data[:,:-1])\n",
    "                err_idx = (data[:,-1] != self.pred_label)\n",
    "\n",
    "                # estim = LogisticRegression(C=0.01,multi_class='multinomial')\n",
    "                # estim.fit(X_train,y_train)\n",
    "                # self.pred_label = estim.predict(data[:,:-1])\n",
    "                # err_idx = (data[:,-1] != self.pred_label)\n",
    "                \n",
    "                self.lrs.append(estim)\n",
    "\n",
    "            else: # Decision Tree\n",
    "                estim = DecisionTreeClassifier(max_depth=2,random_state=rand)\n",
    "                estim.fit(X_train,y_train)\n",
    "                self.pred_label = estim.predict(data[:,:-1])\n",
    "                err_idx = (data[:,-1] != self.pred_label)\n",
    "                self.dcts.append(estim)\n",
    "            \n",
    "            # estimate misclassification from predictions\n",
    "            err = (err_idx*self.probs[:,-1]).sum() + 1e-12     \n",
    "            self.errs = np.hstack((self.errs,err)) # store   \n",
    "            if err >= 1 - 1 / self.num_classes:\n",
    "                break\n",
    "            \n",
    "            # measure performance of the naive bayes with alpha\n",
    "            alpha = np.log((1-err)/err) + np.log(self.num_classes-1)\n",
    "            self.alphas = np.hstack((self.alphas,alpha)) \n",
    "            # print(1.-err_idx.sum()/len(err_idx))\n",
    "            self.pred_labels = np.hstack((self.pred_labels,self.pred_label.reshape((len(data),1))))\n",
    "            # increase weights of the wrongly classified records and decrease weights of the correctly classified records\n",
    "            # training label != prediction label -> misclassified -> e^(alpha) is large -> weight increased\n",
    "            # training label == prediction label -> correctly classified -> e^(0) is 1 -> weight unaffected\n",
    "            # prob = self.probs[:,-1]*np.exp(alpha*err_idx) #/(2*np.sqrt(err*(1-err))) \n",
    "\n",
    "            # training label != prediction label -> misclassified -> e^(alpha) is large -> weight increased\n",
    "            # training label == prediction label -> correctly classified -> e^(-alpha) is small -> weight decreased\n",
    "            prob = np.ones(len(data))\n",
    "            prob[err_idx] = self.probs[err_idx,-1]*np.exp(alpha)\n",
    "            prob[~err_idx] = self.probs[~err_idx,-1]*np.exp(-alpha)\n",
    "            \n",
    "            if prob.sum() <= 0:\n",
    "                print(\"prob sum invalid\")\n",
    "                break     \n",
    "            prob /= prob.sum() # normalize weights   \n",
    "            if ctr+1 >= max_iter:\n",
    "                break\n",
    "            self.probs = np.hstack((self.probs,prob.reshape((len(train),1)))) # store  \n",
    "\n",
    "        self.ctr = ctr+1 # save iterations\n",
    "        # remove garbage initialization values\n",
    "        self.errs = self.errs[1:]\n",
    "        self.alphas = self.alphas[1:]\n",
    "        self.probs = self.probs[:,1:]\n",
    "        self.pred_labels = self.pred_labels[:,1:]\n",
    "    \n",
    "\n",
    "    def predict(self,data):\n",
    "        pred_labels = np.ones((len(data),1))\n",
    "        for ctr in range(len(self.alphas)):\n",
    "            if self.estimator == \"Gaussian\":\n",
    "                pred_label = gaussNB(data,self.means[:,:,ctr],self.vars[:,:,ctr],self.num_classes,self.py[:,ctr])\n",
    "                \n",
    "                # scikit\n",
    "                # pred_label = self.gnbs[ctr].predict(data[:,:-1])\n",
    "\n",
    "            elif self.estimator == \"Multinomial\":\n",
    "                # pred = np.ones((len(data),self.num_classes))\n",
    "                # for i in range(len(data)):\n",
    "                #     pred[i,:] = bayespost(data[i,:-1],self.px[:,:,ctr],self.py[:,ctr])\n",
    "                # pred_label = pred.argmax(1)\n",
    "                \n",
    "                pred_label = self.mnbs[ctr].predict(data[:,:-1])\n",
    "\n",
    "            elif self.estimator == \"Logistic\":\n",
    "                pred_label = self.lrs[ctr].predict(data[:,:-1])\n",
    "\n",
    "            else:\n",
    "                pred_label = self.dcts[ctr].predict(data[:,:-1])\n",
    "\n",
    "            pred_labels = np.hstack((pred_labels,pred_label.reshape((len(data),1)))) \n",
    "        # remove garbage initialization\n",
    "        pred_labels = pred_labels[:,1:]\n",
    "        # adaboost = final classifier is the argmax of the performance weighted sum of predicted values at different iterations\n",
    "        final_pred = sum((pred == np.array([np.arange(self.num_classes)]).T) * alpha \\\n",
    "                        for pred, alpha in zip(pred_labels.T,self.alphas))\n",
    "        # final_pred /= self.alphas.sum()\n",
    "        final_pred = final_pred.argmax(0)\n",
    "        return final_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator:  Gaussian\n",
      "Max iterations: 1\tElapsed iterations: 1\n",
      "Training Accuracy: 62.43%\n",
      "Testing Accuracy: 59.19%\n",
      "\n",
      "Max iterations: 20\tElapsed iterations: 11\n",
      "Training Accuracy: 71.04%\n",
      "Testing Accuracy: 68.88%\n",
      "\n",
      "Max iterations: 50\tElapsed iterations: 11\n",
      "Training Accuracy: 71.04%\n",
      "Testing Accuracy: 68.88%\n",
      "\n",
      "Max iterations: 100\tElapsed iterations: 11\n",
      "Training Accuracy: 71.04%\n",
      "Testing Accuracy: 68.88%\n",
      "\n",
      "Estimator:  Multinomial\n",
      "Max iterations: 1\tElapsed iterations: 1\n",
      "Training Accuracy: 53.82%\n",
      "Testing Accuracy: 49.28%\n",
      "\n",
      "Max iterations: 20\tElapsed iterations: 19\n",
      "Training Accuracy: 54.73%\n",
      "Testing Accuracy: 56.06%\n",
      "\n",
      "Max iterations: 50\tElapsed iterations: 19\n",
      "Training Accuracy: 54.73%\n",
      "Testing Accuracy: 56.06%\n",
      "\n",
      "Max iterations: 100\tElapsed iterations: 19\n",
      "Training Accuracy: 54.73%\n",
      "Testing Accuracy: 56.06%\n",
      "\n",
      "Estimator:  Decision Tree\n",
      "Max iterations: 1\tElapsed iterations: 1\n",
      "Training Accuracy: 59.84%\n",
      "Testing Accuracy: 55.15%\n",
      "\n",
      "Max iterations: 20\tElapsed iterations: 20\n",
      "Training Accuracy: 71.76%\n",
      "Testing Accuracy: 66.06%\n",
      "\n",
      "Max iterations: 50\tElapsed iterations: 50\n",
      "Training Accuracy: 78.78%\n",
      "Testing Accuracy: 73.07%\n",
      "\n",
      "Max iterations: 100\tElapsed iterations: 100\n",
      "Training Accuracy: 82.39%\n",
      "Testing Accuracy: 77.27%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rand = np.random.randint(0,100)\n",
    "M = [1,20,50,100]#,200,500]\n",
    "estims = [\"Gaussian\",\"Multinomial\",\"Decision Tree\",\"Logistic\"]\n",
    "for estim in estims[:-1]:\n",
    "    print(\"Estimator: \", estim)\n",
    "    for m in M:\n",
    "        AB = AdaBoost()\n",
    "        AB.fit(train,dimension,estim,rand,max_iter=m)\n",
    "        print(\"Max iterations: %d\\tElapsed iterations: %d\"%(m,AB.ctr))\n",
    "        \n",
    "        #train\n",
    "        t_pred = AB.predict(train)\n",
    "        err_rate = Loss(train[:,-1],t_pred)\n",
    "        print(f\"Training Accuracy: {100*(1-err_rate):.{4}}%\")\n",
    "\n",
    "        # test\n",
    "        test_pred = AB.predict(test)\n",
    "        err_rate = Loss(test[:,-1],test_pred)\n",
    "        print(f\"Testing Accuracy: {100*(1-err_rate):.{4}}%\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with scikit AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit version uses decision tree classifier as a weak learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator:  Gaussian\n",
      "M:  1\n",
      "Training Accuracy: 61.85%, Testing Accuracy 57.51%\n",
      "\n",
      "M:  20\n",
      "Training Accuracy: 67.21%, Testing Accuracy 64.91%\n",
      "\n",
      "M:  50\n",
      "Training Accuracy: 67.21%, Testing Accuracy 64.91%\n",
      "\n",
      "M:  100\n",
      "Training Accuracy: 67.21%, Testing Accuracy 64.91%\n",
      "\n",
      "Estimator:  Multinomial\n",
      "M:  1\n",
      "Training Accuracy: 27.92%, Testing Accuracy 30.74%\n",
      "\n",
      "M:  20\n",
      "Training Accuracy: 27.92%, Testing Accuracy 30.74%\n",
      "\n",
      "M:  50\n",
      "Training Accuracy: 27.92%, Testing Accuracy 30.74%\n",
      "\n",
      "M:  100\n",
      "Training Accuracy: 27.92%, Testing Accuracy 30.74%\n",
      "\n",
      "Estimator:  Decision Tree\n",
      "M:  1\n",
      "Training Accuracy: 59.77%, Testing Accuracy 54.69%\n",
      "\n",
      "M:  20\n",
      "Training Accuracy: 82.39%, Testing Accuracy 76.13%\n",
      "\n",
      "M:  50\n",
      "Training Accuracy: 85.00%, Testing Accuracy 78.11%\n",
      "\n",
      "M:  100\n",
      "Training Accuracy: 87.83%, Testing Accuracy 81.16%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "est = [GaussianNB(),MultinomialNB(),DecisionTreeClassifier(max_depth=2),LogisticRegression(C=0.01,multi_class='multinomial')]\n",
    "for estim in estims[:-1]:\n",
    "    print(\"Estimator: \", estims[np.where(np.array(estims) == estim)[0][0]])\n",
    "    for m in M:\n",
    "        # train\n",
    "        clf = AdaBoostClassifier(est[np.where(np.array(estims) == estim)[0][0]],n_estimators=m,random_state=rand,algorithm='SAMME')\n",
    "        clf.fit(train[:,:-1], train[:,-1])\n",
    "        t_acc = 100*clf.score(train[:,:-1], train[:,-1])\n",
    "\n",
    "        # test\n",
    "        t_p = clf.predict(test[:,:-1])\n",
    "        te_acc = 100*clf.score(test[:,:-1], test[:,-1])\n",
    "        print(\"M: \",m)\n",
    "        print(\"Training Accuracy: %.2f%%, Testing Accuracy %.2f%%\\n\" % (t_acc, te_acc))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f515e17ece7fdd10b748da9c779c75bf809ff89faf3460578638f0110370c271"
  },
  "kernelspec": {
   "display_name": "Python 3.6.15 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
